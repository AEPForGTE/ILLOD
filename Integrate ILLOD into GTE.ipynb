{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "remarkable-heating",
   "metadata": {},
   "source": [
    "# Intgration of ILLOD into GTE:  Clustering Experiment  (Section 5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welsh-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-milan",
   "metadata": {},
   "source": [
    "## Reading the content of the PROMISE requirements. 30 abbreviations have been inserted in the texts. We want try to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accessory-lending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  set_id\n",
      "1    The system shall refresh the display every 60 ...       1\n",
      "2    The application shall match the color of the s...       1\n",
      "3    If projected  the data must be readable.  On a...       1\n",
      "4    The product shall be available during normal b...       1\n",
      "5    If projected  the data must be understandable....       1\n",
      "..                                                 ...     ...\n",
      "621  User access should be limited to the permissio...      15\n",
      "622  The product must comply with the intranet page...      15\n",
      "623   The intranet pages should display appropriate...      15\n",
      "624  The users should be able to easily use the sys...      15\n",
      "625  The product interface should be fast. The resp...      15\n",
      "\n",
      "[625 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "requirements_data = pd.read_csv('promise_constructed.CSV', names=['text', 'set_id'], sep=';', encoding='utf8')\n",
    "print(requirements_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worldwide-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the data with the aim of storing it in a dictionary\n",
    "data_dict = {}\n",
    "for id_ in set(requirements_data[\"set_id\"]):\n",
    "    sublist = requirements_data[requirements_data[\"set_id\"] == id_]\n",
    "    data_dict[id_] = [req for req in sublist[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revised-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reqs = data_dict[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-discretion",
   "metadata": {},
   "source": [
    "## NC and Abbreviation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-burden",
   "metadata": {},
   "source": [
    "### Helper Functions to extract noun chunks (NCs) and abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cooked-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"the\", \"and\", \"i\", \"for\", \"as\", \"an\", \"a\", \"if\", \"any\", \"all\", \"one\", \"on\", \"new\", \"out\", \"we\", \"to\", \"at\", \"by\", \"from\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portion_of_capital_letters(w):\n",
    "    upper_cases = ''.join([c for c in w if c.isupper()])\n",
    "    return len(upper_cases)/len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quarterly-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbv_detect(sent):\n",
    "    abv = set()\n",
    "    for word in sent.split():\n",
    "        if (len(word) <= 13 and portion_of_capital_letters(word) >= 0.29):\n",
    "            if len([c for c in word if c.isupper()]) == 1 and word[0].isupper() and word.lower() in stop_words:\n",
    "                continue\n",
    "            abv.add(word.strip(punctuation))\n",
    "    return abv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affiliated-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_nc(nc):\n",
    "    doc = nlp(nc)\n",
    "    cleaned_nc = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"DET\":\n",
    "            cleaned_nc = cleaned_nc + \" \" + token.lemma_\n",
    "            cleaned_nc = re.sub(r\"[\\([{})\\]]\", \"\", cleaned_nc)\n",
    "            cleaned_nc = cleaned_nc.strip()\n",
    "    return cleaned_nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-above",
   "metadata": {},
   "source": [
    "Extraction of noun chunks according to [2] (Arora, Chetan, et al. \"Automated extraction and clustering of requirements glossary terms.\" IEEE Transactions on Software Engineering 43.10 (2016): 918-945). Some Pos-Tag-Patterns are added to the NC detection to improve recall of spacy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sublime-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_detect(req):\n",
    "    noun_chunks_set = set()\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern1 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "    pattern2 = [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    pattern3 = [{'POS': 'NOUN'}, {'POS': 'DET'}, {'POS': 'NOUN'}]\n",
    "    pattern4 = [{'POS': 'NOUN'}, {'POS': 'PROPN'}]\n",
    "    pattern5 = [{'POS': 'NOUN'}]\n",
    "    matcher.add(\"TrigramNCs\", [pattern1, pattern2, pattern3, pattern4, pattern5])\n",
    "    doc = nlp(req)\n",
    "    matches = matcher(doc)\n",
    "    for nc_ in doc.noun_chunks:\n",
    "        noun_chunks_set.add(nc_.text)\n",
    "    \n",
    "\n",
    "    composed_terms = set()\n",
    "    for nc1 in noun_chunks_set:\n",
    "        for nc2 in noun_chunks_set:\n",
    "            comp_term1 = nc1 + \" of \" + nc2\n",
    "            comp_term2 = nc1 + \" and \" + nc2\n",
    "            if comp_term1 in req:\n",
    "                composed_terms.add(comp_term1)\n",
    "            if comp_term2 in req:\n",
    "                composed_terms.add(comp_term2)\n",
    "    found_terms = noun_chunks_set.union(composed_terms)\n",
    "    \n",
    "    cleaned_terms = []\n",
    "    for t in found_terms:\n",
    "        cleaned_terms.append(normalize_nc(t))\n",
    "    return set(cleaned_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-borough",
   "metadata": {},
   "source": [
    "## ILLOD with its methods (Section 4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polished-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_special_characters(s1, s2):\n",
    "    invalidcharacters = set(string.punctuation)\n",
    "    if any(char in invalidcharacters for char in s1):\n",
    "        s1_ = s1.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        s1_ = s1\n",
    "    if any(char in invalidcharacters for char in s2):\n",
    "        s2_ = s2.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        s2_ = s2\n",
    "    return s1_, s2_\n",
    "\n",
    "def stop_words_handling(term):\n",
    "    splitted_term = term.split()\n",
    "    stop_words = set([\"for\", \"and\", \"of\", \"in\", \"via\", \"be\"])\n",
    "    \n",
    "    if splitted_term[0] in stop_words:\n",
    "        stop_words = stop_words - set([splitted_term[0]])\n",
    "                \n",
    "    for sw in stop_words:\n",
    "        while sw in splitted_term:\n",
    "            splitted_term.remove(sw)\n",
    "    sanitized_term = \" \".join([w for w in splitted_term]) \n",
    "        \n",
    "    return sanitized_term\n",
    "\n",
    "def clean_string_pair_and_reduce_expansion(abb, term):\n",
    "    abb_lower = abb.lower()\n",
    "    term_lower = term.lower()\n",
    "    sanitized_abbv, sanitized_term = clear_special_characters(abb_lower, term_lower) \n",
    "    sanitized_term_without_stopswords = stop_words_handling(sanitized_term)\n",
    "    initial_letters_of_tokens_of_sanitized_term_without_stopswords = ''.join([c[0] for c in sanitized_term_without_stopswords.split()])\n",
    "    return sanitized_abbv, initial_letters_of_tokens_of_sanitized_term_without_stopswords\n",
    "\n",
    "def check_initial_letters(a, t):\n",
    "    initial_letters_of_tokens_of_t = ''.join([c[0] for c in t.split()])\n",
    "    if initial_letters_of_tokens_of_t == a or initial_letters_of_tokens_of_t.upper() == a:\n",
    "        return True\n",
    "    \n",
    "def check_length_consistency(a, t):\n",
    "    length_consistency = False\n",
    "    if len(t.split()) <= len(a):\n",
    "        length_consistency = True\n",
    "    return length_consistency\n",
    "\n",
    "def check_order(a, t):\n",
    "    abbv_reversed = a.lower()[::-1]\n",
    "    term_reversed = t.lower()[::-1]\n",
    "    len_of_term = len(t)\n",
    "    \n",
    "    pos_memory = 0\n",
    "    pos_memory_list = []\n",
    "    order_matching_string_rev = \"\"\n",
    "    \n",
    "    for j, char_from_abbv in enumerate(abbv_reversed):\n",
    "        if j == len(abbv_reversed) - 1 and len(pos_memory_list) > 0 and pos_memory == len(term_reversed):\n",
    "            break\n",
    "        else:\n",
    "            for i, char_from_term in enumerate(term_reversed[pos_memory:]):\n",
    "                if char_from_abbv == char_from_term:\n",
    "                    order_matching_string_rev = order_matching_string_rev + char_from_abbv\n",
    "                    pos_memory = pos_memory + i + 1\n",
    "                    pos_memory_list.append(len_of_term - pos_memory)\n",
    "                    break\n",
    "    if order_matching_string_rev == abbv_reversed:\n",
    "        return True, pos_memory_list[::-1]\n",
    "    else:\n",
    "        return False, []\n",
    "\n",
    "def check_distribution_of_matching_characters(pos_of_chars_list, t):\n",
    "    term_intervals = []\n",
    "    len_of_term = len(t)\n",
    "    i = 0\n",
    "    while i < len_of_term:\n",
    "        sublist = []\n",
    "        j = i\n",
    "        while j < len_of_term and t[j] != \" \":\n",
    "            sublist.append(j)\n",
    "            j = j+ 1\n",
    "        i = j+1\n",
    "        term_intervals.append(sublist)\n",
    "        \n",
    "    splitted_term = t.split()      \n",
    "    \n",
    "    containment_list = []\n",
    "    for i, interval in enumerate(term_intervals):\n",
    "        contanment_sublist = []\n",
    "        for pos in pos_of_chars_list:\n",
    "            if (pos in interval) and (splitted_term[i][0] == t[pos]):\n",
    "                contanment_sublist.append(0)\n",
    "            elif pos in interval:\n",
    "                contanment_sublist.append(interval.index(pos))\n",
    "        if len(contanment_sublist) == 0:\n",
    "            contanment_sublist.append(-1)\n",
    "        containment_list.append(contanment_sublist)\n",
    "    \n",
    "    result_of_distribution_check = False\n",
    "    if len(containment_list) <= 1:\n",
    "        result_of_distribution_check = True\n",
    "    elif len (containment_list) >= 2:\n",
    "        non_zero_count = 0\n",
    "        for sublist in containment_list[1:]:\n",
    "            if len(sublist) == 1 and 0 not in sublist:\n",
    "                non_zero_count += 1\n",
    "        if non_zero_count == 0:\n",
    "            result_of_distribution_check = True\n",
    "    \n",
    "    return result_of_distribution_check\n",
    "\n",
    "\n",
    "def illod(abbv, term, threshold=None):\n",
    "    if (abbv[0].lower() == term[0].lower()):\n",
    "        \n",
    "        \n",
    "        ###################################### Step (a) ##########################################\n",
    "        # check wether initial letters of tokens in t match with the letters in abbreviation\n",
    "        if check_initial_letters(abbv, term):\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################### Step (b) ##########################################\n",
    "        # clean abbreviation and term from special characters and stopwords\n",
    "        a_, t_ = clean_string_pair_and_reduce_expansion(abbv, term)\n",
    "        if a_ == t_:\n",
    "            return True\n",
    "        \n",
    "        sanitized_abbv, sanitized_term = clear_special_characters(abbv, term) \n",
    "        sanitized_term_without_stopswords = stop_words_handling(sanitized_term)\n",
    "        sanitized_term_without_stopswords_splitted  = sanitized_term_without_stopswords.split()\n",
    "        \n",
    "        ###################################### Step (c), (d), (e) #################################\n",
    "        # Sequential call of the methods that check and compare lengths, order and distribution of characters\n",
    "        length_consistency = check_length_consistency(sanitized_abbv, sanitized_term_without_stopswords)\n",
    "        order, pos_of_chars_list = check_order(sanitized_abbv, sanitized_term_without_stopswords)\n",
    "        distribution = check_distribution_of_matching_characters(pos_of_chars_list, sanitized_term_without_stopswords)\n",
    "\n",
    "\n",
    "        if length_consistency and order and distribution:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        ################################## in case first letter differs ###########################\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-egyptian",
   "metadata": {},
   "source": [
    "## Helper Functions for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "about-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(keyword):\n",
    "    return nltk.word_tokenize(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "roman-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_dict (prediction_list, terms_list):\n",
    "    dict_for_mapping = {}\n",
    "    for i, cluster_id in enumerate(prediction_list):\n",
    "        if int(cluster_id) in dict_for_mapping:\n",
    "            tmp = dict_for_mapping[int(cluster_id)]\n",
    "            tmp.append(terms_list[i])\n",
    "            dict_for_mapping[int(cluster_id)] = tmp\n",
    "        else:\n",
    "            dict_for_mapping[int(cluster_id)] = [terms_list[i]]\n",
    "    return dict_for_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-potato",
   "metadata": {},
   "source": [
    "### The following function performs a set transformation. It distributes objects of the sets $A$ and $T$ to the sets $OT$ and $G^{a}$ according to steps (4) and (6) from section (5.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "colored-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sets_for_term_types(set_of_abbreviations, set_of_terms):\n",
    "    \n",
    "    #compliant wit section 5.2: terms_that_contain_abbreviations = T \\ OT\n",
    "    terms_that_contain_abbreviations = set()\n",
    "    \n",
    "    for term in set_of_terms:\n",
    "        for abb in set_of_abbreviations:\n",
    "            if abb in term.split():\n",
    "                terms_that_contain_abbreviations.add(term)\n",
    "    \n",
    "    ordinary_terms = terms - terms_that_contain_abbreviations\n",
    "    \n",
    "    return ordinary_terms, terms_that_contain_abbreviations       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-tiffany",
   "metadata": {},
   "source": [
    "## Generation of Clusters according to steps(1) till step(8) (Section 5.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sorted-restaurant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['audit report', 'damaged vehicle part information', 'original search result', '98 % uptime', '2 year', 'search result', 'save', 'list', 'initial launch', 'search', 'audit', 'service', 'zipcode', 'approximately 1:00 am', 'denial', 'supplied vehicle part', 'application', 'access', 'supervisor role', 'supplied vehicle part and supplier', 'vehicle vehicle location', 'vehicle year', 'percentage', '2 year of initial launch', 'denial of service', 'supplier', 'day', 'radius', 'environment', 'search radius', 'indivual line item', 'feed', 'miles', 'state', 'scale', '85 %', 'category', 'vehicle datum', '90 %', '98 %', '1 month', 'more than 2 %', 'city', 'vehicle location', 'training', 'instruction', 'middleware technology team']\n",
      "############################################################\n",
      "14\n",
      "['average number', 'number', 'total number of recycled part', 'total score of audit', 'total number', 'total score']\n",
      "############################################################\n",
      "7\n",
      "['80 % of Collision Estimators', 'productivity of Collision Estimators', '80 %', 'productivity']\n",
      "############################################################\n",
      "6\n",
      "['system', 'ChoiceParts system', '90 % of system', 'Choice part System']\n",
      "############################################################\n",
      "5\n",
      "['maintenance of product', 'maintenance', 'appearance', 'appearance of product', 'product', 'product installation']\n",
      "############################################################\n",
      "1\n",
      "['list of repair facility', 'repair facility rating', 'preferred repair facility rating', 'current repair facility rating', 'new rating', 'high rating', 'rating', 'preferred repair facility', 'repair facility', 'blank set of rating']\n",
      "############################################################\n",
      "2\n",
      "['available recycled part information', 'part', 'preferred part supplier', 'feed of recycled part datum', 'available recycled part information and supplier', 'percentage of available recycled part', 'recycled part audit report', 'recycled part record', 'average number of recycled part record', 'recycled part', 'select recycled part', 'available recycled part and supplier', 'recycled part search result', 'recycled part audit', 'list of preferred part supplier', 'recycled part audits', 'available part', 'recycled part datum', 'available recycled part', 'number of available recycled part']\n",
      "############################################################\n",
      "13\n",
      "['invalid datum', 'claim processing', 'one insurance company', 'insurance companys claim datum', 'only valid datum', 'insurance regulation']\n",
      "############################################################\n",
      "11\n",
      "['8 second', '10 second', 'no long than 15 second', '15 second', '5 second']\n",
      "############################################################\n",
      "8\n",
      "['only collision estimator', 'estimate assignment', 'collision estimator role', 'available recycled part and collision estimate', 'estimator', 'collision estimate', 'collision estimator', 'recycled part audit of collision estimate', 'estimate']\n",
      "############################################################\n",
      "0\n",
      "['98 % of schedule outage', 'input criterion', 'two day', 'schedule outage', 'corporate online availability schedule', 'malicious attack', 'street address', 'internet explorer', 'computer virus', 'blank set', 'availability schedule', 'Mozilla Firefox', 'Sarbanes - Oxley', 'attempt use', 'exist hardware', 'time', 'more than 2 % of available online time', 'estimatic law', 'later point', 'available online time', 'establish launch time frame']\n",
      "############################################################\n",
      "3\n",
      "['95 %', 'other adjuster', 'adjuster', 'adjuster role', 'only adjuster', '95 % of adjuster']\n",
      "############################################################\n",
      "9\n",
      "['corporate color scheme', '85 % of user', 'corporate Architecture guideline', '10 000 concurrent user', 'establish corporate maintenance window', 'corporate support center', 'User help', 'corporate User Interface Guidelines', 'user', 'up to 1500 simultaneous user', '1500 user']\n",
      "############################################################\n",
      "15\n",
      "['upgrade', 'product installation and upgrade']\n",
      "############################################################\n",
      "12\n",
      "['radius of 30 mile', '30 mile', '1 and 30 mile']\n",
      "############################################################\n",
      "10\n",
      "['rp and actual use', 'Collision Estimators', 'adjuster and Collision Estimators', 'dirty and noisy condition']\n",
      "############################################################\n",
      "AR\n",
      "['audit report', 'adjuster', 'appearance', 'adjuster role', 'available part', 'AR']\n",
      "############################################################\n",
      "rP\n",
      "['recycled part', 'rp and actual use of rP', 'attempt use of rP and actual use', 'attempt use of rP', 'rP']\n",
      "############################################################\n",
      "RF\n",
      "['repair facility', 'prefer RF', 'RF']\n",
      "############################################################\n",
      "CE\n",
      "['claim processing', 'Collision Estimators', 'category', 'collision estimate', 'collision estimator', 'CE']\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "######## Step(1) + Step(3): Extract set of Abbreviations A and set of terms T ############\n",
    "terms = set()\n",
    "abbv_set = set()\n",
    "for req in test_reqs:\n",
    "    terms = terms.union(nc_detect(req))\n",
    "    abbv_set = abbv_set.union(abbv_detect(req))\n",
    "\n",
    "\n",
    "########## placeholder for step(2): Reduce extracted abbreviations set A through #########\n",
    "#########  cmparision with project resources so that only undefined abbreviations stay in A\n",
    "\n",
    "\n",
    "###################### step(4): determine the sets A, OT and T\\OT ########################\n",
    "ordinary_terms, terms_that_contain_abbs = determine_sets_for_term_types(abbv_set, terms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################### step(5): ##########################################\n",
    "AEP_candidate_clusters = {}\n",
    "abbreviations_with_matching_candidates = set()\n",
    "for abv in abbv_set:\n",
    "    for term in ordinary_terms:\n",
    "        if illod(abv, term):\n",
    "            abbreviations_with_matching_candidates.add(abv)\n",
    "            if abv in AEP_candidate_clusters:\n",
    "                expansion_candidates_list = AEP_candidate_clusters[abv]\n",
    "                expansion_candidates_list.append(term)\n",
    "                AEP_candidate_clusters[abv] = expansion_candidates_list\n",
    "            else:\n",
    "                AEP_candidate_clusters[abv] = [term]\n",
    "\n",
    "######################################### step(6): #######################################\n",
    "for abv in abbreviations_with_matching_candidates:\n",
    "    for term in terms_that_contain_abbs:\n",
    "        if abv in term.split() and abv != term:\n",
    "            if abv in AEP_candidate_clusters:\n",
    "                expansion_candidates_list = AEP_candidate_clusters[abv]\n",
    "                expansion_candidates_list.append(term)\n",
    "                AEP_candidate_clusters[abv] = expansion_candidates_list\n",
    "            else:\n",
    "                AEP_candidate_clusters[abv] = [term]\n",
    "\n",
    "####################### step(7): generate clusters for terms from OT #####################\n",
    "tfidf = TfidfVectorizer()\n",
    "X = pd.DataFrame(tfidf.fit_transform(ordinary_terms).toarray(),\n",
    "                 index=list(ordinary_terms), columns=tfidf.get_feature_names())\n",
    "cluster_dict = {}\n",
    "gmm = GaussianMixture(n_components=16).fit(X)\n",
    "pred = gmm.predict(X)\n",
    "cluster_dict = create_cluster_dict(pred, list(ordinary_terms))\n",
    "\n",
    "\n",
    "\n",
    "## step(8): Add AEP groups as additional clusters to the clusters of the ordinary terms ##\n",
    "for key in  AEP_candidate_clusters:\n",
    "    tmp_list = AEP_candidate_clusters[key]\n",
    "    tmp_list.append(key)\n",
    "    cluster_dict[key] = tmp_list\n",
    "\n",
    "    \n",
    "################################# print on terminal ######################################\n",
    "for key in cluster_dict.keys():\n",
    "    print(str(key))\n",
    "    print(cluster_dict[key])\n",
    "    print(\"############################################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
