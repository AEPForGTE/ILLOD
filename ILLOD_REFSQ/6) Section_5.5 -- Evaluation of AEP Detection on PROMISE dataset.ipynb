{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooked-symphony",
   "metadata": {},
   "source": [
    "# Evaluation of the Different Approaches to AEP Detection on the Promise dataset (Section 5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Syntactic_Classifiers\n",
    "import ILLOD\n",
    "import Abbreviation_and_NC_Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-natural",
   "metadata": {},
   "source": [
    "## Reading the content of the PROMISE requirements. 30 abbreviations have been inserted in the texts. We try to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cutting-leeds",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "requirements_data = pd.read_csv('Promise_constructed.CSV', names=['text', 'set_id'], sep=';', encoding='utf8')\n",
    "# print(requirements_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "miniature-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the data with the aim of storing it in a dictionary\n",
    "data_dict = {}\n",
    "for id_ in set(requirements_data[\"set_id\"]):\n",
    "    sublist = requirements_data[requirements_data[\"set_id\"] == id_]\n",
    "    data_dict[id_] = [req for req in sublist[\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-surrey",
   "metadata": {},
   "source": [
    "## The following function performs a set transformation. It partitions objects of the set  ùëá  to the sets  ùëÇùëá  and  ùëá‚àñùëÇùëá according to steps (4) and (6) from section (6.2, Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incident-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sets_for_term_types(set_of_abbreviations, set_of_terms):\n",
    "    \n",
    "    #compliant wit section 5.2: terms_that_contain_abbreviations = T \\ OT\n",
    "    terms_that_contain_abbreviations = set()\n",
    "    \n",
    "    for term in set_of_terms:\n",
    "        for abb in set_of_abbreviations:\n",
    "            if abb in term.split():\n",
    "                terms_that_contain_abbreviations.add(term)\n",
    "    \n",
    "    ordinary_terms = set_of_terms - terms_that_contain_abbreviations\n",
    "    \n",
    "    return ordinary_terms, terms_that_contain_abbreviations              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-creek",
   "metadata": {},
   "source": [
    "## Here we generate AEP Candidates and AEP groups with the different approches/ aep_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "special-apparatus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_aep_candidates_and_groups(aep_classifier):\n",
    "    counter = 0\n",
    "    overall_aep_candidates = []\n",
    "    AEP_candidate_clusters ={}\n",
    "    for id_ in data_dict.keys():\n",
    "\n",
    "        ######### Step(1) + Step(3): Extract set of Abbreviations A and set of terms T############\n",
    "\n",
    "        terms = set()\n",
    "        abbv_set = set()\n",
    "        # print(\"TUPLES FROM ReqSet: \" + str(id_))\n",
    "        for req in data_dict[id_]:\n",
    "            terms = terms.union(Abbreviation_and_NC_Extraction.nc_detect(req))\n",
    "            abbv_set = abbv_set.union(Abbreviation_and_NC_Extraction.abbv_detect(req))\n",
    "\n",
    "        ############ step(2): Reduce extracted abbreviations set A through cmparision with #######\n",
    "        ############ project resources so that only undefined abbreviations stay in A ############\n",
    "\n",
    "\n",
    "\n",
    "        ############################ step(4): determine the sets A, OT and T\\OT ##################\n",
    "        ordinary_terms, terms_that_contain_abbs = determine_sets_for_term_types(abbv_set, terms)\n",
    "\n",
    "\n",
    "        # For every a‚àà A generate an AEP group G^{a} of possible expansions t ‚àà OT via ILLOD.####\n",
    "        ###################################### step(5): ##########################################\n",
    "        abbreviations_with_matching_candidates = set()\n",
    "        for abv in abbv_set:\n",
    "            for term in ordinary_terms:\n",
    "                if aep_classifier(abv, term):\n",
    "                    overall_aep_candidates.append((abv, term.lower()))\n",
    "                    counter += 1\n",
    "                    abbreviations_with_matching_candidates.add(abv)\n",
    "                    #print(str(counter)+ \") (\" + abv + \", \" + term + \")\")\n",
    "                    if id_ in AEP_candidate_clusters.keys():\n",
    "                        if abv in AEP_candidate_clusters[id_]:\n",
    "                            expansion_candidates_list = AEP_candidate_clusters[id_][abv]\n",
    "                            expansion_candidates_list.append(term)\n",
    "                            AEP_candidate_clusters[id_][abv] = expansion_candidates_list\n",
    "                        else:\n",
    "                            AEP_candidate_clusters[id_][abv] = [term]\n",
    "                    else:\n",
    "                        AEP_candidate_clusters[id_] = {}\n",
    "\n",
    "        # For every a‚àà A extend its G^{a} with terms t‚àà T\\OT, if t contains a.####################\n",
    "        ######################################### step(6): #######################################\n",
    "        for abv in abbreviations_with_matching_candidates:\n",
    "            for term in terms_that_contain_abbs:\n",
    "                if abv in term.split() and abv != term:\n",
    "                    if id_ in AEP_candidate_clusters.keys():\n",
    "                        if abv in AEP_candidate_clusters[id_]:\n",
    "                            expansion_candidates_list = AEP_candidate_clusters[id_][abv]\n",
    "                            expansion_candidates_list.append(term)\n",
    "                            AEP_candidate_clusters[id_][abv] = expansion_candidates_list\n",
    "                        else:\n",
    "                            AEP_candidate_clusters[id_][abv] = [term]\n",
    "                    else:\n",
    "                        AEP_candidate_clusters[id_] = {}\n",
    "        #print(\"#####################################################\")\n",
    "    print(\"number of aep candidates: \" + str(counter))\n",
    "\n",
    "    cluster_counter = 0\n",
    "    for id_ in AEP_candidate_clusters.keys():\n",
    "        # print(\"CLUSTERS FROM ReqSet: \" + str(id_) + \":\")\n",
    "        for key in AEP_candidate_clusters[id_]:\n",
    "            cluster_counter += 1\n",
    "            # print(str(cluster_counter) + \") \" + \"\\\"\" + str(key)+ \"\\\"\" + \" : \" + str(AEP_candidate_clusters[id_][key]))\n",
    "        # print(\"#####################################################\")\n",
    "    print(\"number of aep groups: \" + str(cluster_counter))\n",
    "    return overall_aep_candidates\n",
    "\n",
    "def evaluate_aep_detection_approach(aep_classifier):\n",
    "    tuple_collection_for_evaluation = generate_aep_candidates_and_groups(aep_classifier)\n",
    "    inserted_abbreviations = pd.read_csv('insertedAbbreviations.txt', names=['expansion', 'abbreviation'], sep='\\t', encoding='utf8')\n",
    "    inserted_abbs = inserted_abbreviations[\"abbreviation\"].tolist()\n",
    "    inserted_exp = inserted_abbreviations[\"expansion\"].tolist()\n",
    "    inserted_abbreviations_as_tuples = [(inserted_abbs[i], inserted_exp[i]) for i in range(0, len(inserted_exp))]\n",
    "\n",
    "    def contains_exp(proper_tuple, tup):\n",
    "        cont = True\n",
    "        for t in proper_tuple[1].split():\n",
    "            exp_candidate_lower = tup[1].lower()\n",
    "            if t not in exp_candidate_lower.split():\n",
    "                cont = False\n",
    "        if cont:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    matches_for_final_eval = 0\n",
    "    resolved_abbs = set()\n",
    "    print(\"FOUND AEPs:\")\n",
    "    for tup in tuple_collection_for_evaluation:\n",
    "        for proper_tuple in inserted_abbreviations_as_tuples:\n",
    "            if tup == proper_tuple and contains_exp(proper_tuple, tup):\n",
    "                if tup[0] not in resolved_abbs:\n",
    "                    print(tup)\n",
    "                    resolved_abbs.add(tup[0])\n",
    "                    matches_for_final_eval += 1\n",
    "    print(matches_for_final_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-scholar",
   "metadata": {},
   "source": [
    "## Result of the Abbreviation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opposite-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERTED ABBREVIATIONS THAT WERE FOUND: \n",
      "['LeSco', 'PoS', 'PMs', 'NSM', 'RT', 'IQA', 'CR', 'CC', 'sMo', 'DS', 'Sys', 'dG', 'CE', 'PF', 'Csi', 'SP', 'cT', 'DC', 'sI', 'TR', 'STAT', 'WES', 'AR', 'oP', 'RF', 'LeDA', 'SR', 'rP', 'UI']\n",
      "Number of detected abbreviations = 29\n"
     ]
    }
   ],
   "source": [
    "abbeviations_set = set()\n",
    "for id_ in data_dict.keys():\n",
    "    for req in data_dict[id_]:\n",
    "        abbeviations_set = abbeviations_set.union(Abbreviation_and_NC_Extraction.abbv_detect(req))\n",
    "\n",
    "inserted_abbreviations = pd.read_csv('insertedAbbreviations.txt', names=['expansion', 'abbreviation'], sep='\\t', encoding='utf8')\n",
    "inserted_abbs = set(inserted_abbreviations[\"abbreviation\"].tolist())\n",
    "\n",
    "intersection = [abv for abv in abbeviations_set if abv in inserted_abbs]\n",
    "print(\"INSERTED ABBREVIATIONS THAT WERE FOUND: \")\n",
    "print(intersection)\n",
    "print(\"Number of detected abbreviations = \" + str(len(intersection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-discipline",
   "metadata": {},
   "source": [
    "## Main Program to count the number of generated AEP candidates, AEP groups and to show corectly detected AEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "emotional-longitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of aep candidates: 115\n",
      "number of aep groups: 51\n",
      "FOUND AEPs:\n",
      "('cT', 'current time')\n",
      "('SR', 'search result')\n",
      "('RT', 'realtor')\n",
      "('Csi', 'clinical site')\n",
      "('DC', 'dispute case')\n",
      "('DS', 'disputes system')\n",
      "('NSM', 'nursing staff member')\n",
      "('RF', 'repair facility')\n",
      "('rP', 'recycled part')\n",
      "('AR', 'audit report')\n",
      "('CE', 'collision estimate')\n",
      "('CR', 'conference room')\n",
      "('SP', 'search parameter')\n",
      "('sI', 'substitutionary ingredient')\n",
      "('PF', 'product formula')\n",
      "('IQA', 'inventory quantity adjustment')\n",
      "('CC', 'credit card')\n",
      "('sMo', 'streaming movie')\n",
      "('Sys', 'system')\n",
      "('LeSco', 'lead score')\n",
      "('WES', 'web service')\n",
      "('LeDA', 'lead data')\n",
      "('STAT', 'status')\n",
      "('oP', 'offensive player')\n",
      "('dG', 'defensive grid')\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ILLOD\n",
    "evaluate_aep_detection_approach(ILLOD.illod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "entitled-metallic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of aep candidates: 870\n",
      "number of aep groups: 71\n",
      "FOUND AEPs:\n",
      "('cT', 'current time')\n",
      "('SR', 'search result')\n",
      "('RT', 'realtor')\n",
      "('Csi', 'clinical site')\n",
      "('DC', 'dispute case')\n",
      "('DS', 'disputes system')\n",
      "('NSM', 'nursing staff member')\n",
      "('RF', 'repair facility')\n",
      "('rP', 'recycled part')\n",
      "('AR', 'audit report')\n",
      "('CE', 'collision estimate')\n",
      "('CR', 'conference room')\n",
      "('SP', 'search parameter')\n",
      "('sI', 'substitutionary ingredient')\n",
      "('PF', 'product formula')\n",
      "('IQA', 'inventory quantity adjustment')\n",
      "('CC', 'credit card')\n",
      "('sMo', 'streaming movie')\n",
      "('WES', 'web service')\n",
      "('oP', 'offensive player')\n",
      "('dG', 'defensive grid')\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Levenshtein-Distance_Classifier\n",
    "evaluate_aep_detection_approach(Syntactic_Classifiers.ld_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "built-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of aep candidates: 251\n",
      "number of aep groups: 62\n",
      "FOUND AEPs:\n",
      "('cT', 'current time')\n",
      "('SR', 'search result')\n",
      "('RT', 'realtor')\n",
      "('Csi', 'clinical site')\n",
      "('DC', 'dispute case')\n",
      "('DS', 'disputes system')\n",
      "('NSM', 'nursing staff member')\n",
      "('RF', 'repair facility')\n",
      "('rP', 'recycled part')\n",
      "('AR', 'audit report')\n",
      "('CE', 'collision estimate')\n",
      "('CR', 'conference room')\n",
      "('SP', 'search parameter')\n",
      "('sI', 'substitutionary ingredient')\n",
      "('PF', 'product formula')\n",
      "('IQA', 'inventory quantity adjustment')\n",
      "('CC', 'credit card')\n",
      "('sMo', 'streaming movie')\n",
      "('LeSco', 'lead score')\n",
      "('LeDA', 'lead data')\n",
      "('oP', 'offensive player')\n",
      "('dG', 'defensive grid')\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Jaro-Winkler-Similarity_Classifier\n",
    "evaluate_aep_detection_approach(Syntactic_Classifiers.jws_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rational-catch",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of aep candidates: 258\n",
      "number of aep groups: 66\n",
      "FOUND AEPs:\n",
      "('cT', 'current time')\n",
      "('SR', 'search result')\n",
      "('Csi', 'clinical site')\n",
      "('DC', 'dispute case')\n",
      "('DS', 'disputes system')\n",
      "('NSM', 'nursing staff member')\n",
      "('RF', 'repair facility')\n",
      "('rP', 'recycled part')\n",
      "('AR', 'audit report')\n",
      "('CE', 'collision estimate')\n",
      "('CR', 'conference room')\n",
      "('SP', 'search parameter')\n",
      "('sI', 'substitutionary ingredient')\n",
      "('PF', 'product formula')\n",
      "('IQA', 'inventory quantity adjustment')\n",
      "('CC', 'credit card')\n",
      "('sMo', 'streaming movie')\n",
      "('WES', 'web service')\n",
      "('oP', 'offensive player')\n",
      "('dG', 'defensive grid')\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Dice_Coefficient-Classiifer\n",
    "evaluate_aep_detection_approach(Syntactic_Classifiers.dc_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
