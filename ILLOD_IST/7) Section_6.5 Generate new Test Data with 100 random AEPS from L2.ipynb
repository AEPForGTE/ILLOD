{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550abc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NC-Extraction + Generation of new validation data (100 uncontrolled abbreviations in PURE dataset)\n",
    "\n",
    "\n",
    "def normalize_nc(nc):\n",
    "    doc = nlp(nc)\n",
    "    cleaned_nc = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"DET\" or token.text in [\"such\", \"\"]:\n",
    "            cleaned_nc = cleaned_nc + \" \" + token.lemma_\n",
    "            cleaned_nc = re.sub(r\"[\\([{})\\]]\", \"\", cleaned_nc)\n",
    "            cleaned_nc = cleaned_nc.strip()\n",
    "    return cleaned_nc\n",
    "\n",
    "def nc_detect(req):\n",
    "    noun_chunks_set = set()\n",
    "    doc = nlp(req)\n",
    "    for nc_ in doc.noun_chunks:\n",
    "        noun_chunks_set.add(nc_.text)\n",
    "\n",
    "    composed_terms = set()\n",
    "    for nc1 in noun_chunks_set:\n",
    "        for nc2 in noun_chunks_set:\n",
    "            comp_term1 = nc1 + \" of \" + nc2\n",
    "            comp_term2 = nc1 + \" and \" + nc2\n",
    "            if comp_term1 in req:\n",
    "                composed_terms.add(comp_term1)\n",
    "            if comp_term2 in req:\n",
    "                composed_terms.add(comp_term2)\n",
    "    found_terms = noun_chunks_set.union(composed_terms)\n",
    "  \n",
    "    term_pairs = []\n",
    "    for original_term in found_terms:\n",
    "        term_pairs.append((original_term, normalize_nc(original_term)))\n",
    "    return set(term_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_nc_to_reqID_map(data_list):\n",
    "    nc_to_reqID_map = {}\n",
    "    for sample in data_list[1:]:\n",
    "        current_ID = sample[1] + \"_\" + sample[0]\n",
    "        set_of_nc_tuples_in_sent = nc_detect(sample[2])\n",
    "\n",
    "        for term_tuple in set_of_nc_tuples_in_sent:\n",
    "            term = term_tuple[1]\n",
    "            if term in nc_to_reqID_map.keys():\n",
    "                ID_list_so_far = nc_to_reqID_map[term]\n",
    "                if current_ID not in ID_list_so_far:\n",
    "                    ID_list_so_far.append(current_ID)\n",
    "                nc_to_reqID_map[term] = ID_list_so_far\n",
    "            else:\n",
    "                nc_to_reqID_map[term] = [current_ID]\n",
    "    return nc_to_reqID_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_term_with_abb_in_given_req (req_text, cleaned_nc, abb):\n",
    "    set_of_term_pairs = nc_detect(req_text)\n",
    "    req_text_=\"\"\n",
    "    for item in set_of_term_pairs:\n",
    "        if item[1] == cleaned_nc:\n",
    "            req_text_ = req_text.replace(item[0], abb)\n",
    "            break\n",
    "    return req_text_\n",
    "\n",
    "\n",
    "def replace_phrase_with_abb(nc_to_reqID_map, reqs_list, replacement_sample):\n",
    "    list_of_term_occurances = nc_to_reqID_map[replacement_sample[0]]\n",
    "    rand_int = random.randint(1, len(list_of_term_occurances)-1)\n",
    "    random_pick_of_occurances = random.choices(list_of_term_occurances, k=rand_int)\n",
    "    changed_data_dict = {}\n",
    "    \n",
    "    for _id in random_pick_of_occurances:\n",
    "        for req in reqs_list:\n",
    "            if req[1] + \"_\" +req[0] == _id:\n",
    "                _new_req = replace_term_with_abb_in_given_req(req[2], replacement_sample[0], replacement_sample[1])\n",
    "                changed_data_dict[_id] = _new_req\n",
    "    \n",
    "    updated_list = []\n",
    "    for req in reqs_list:\n",
    "        if req[1] + \"_\" +req[0] not in changed_data_dict.keys():\n",
    "            updated_list.append(req)\n",
    "        else:\n",
    "            updated_list.append([req[0], req[1], changed_data_dict[req[1] + \"_\" +req[0]]])\n",
    "    return updated_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_uncontrolled_abbreviations_in_requirements(data_list, terms_to_be_replaced):\n",
    "    nc_to_reqID_map = generate_nc_to_reqID_map(data_list)\n",
    "    list_of_replacements = []\n",
    "    for sample in terms_to_be_replaced:\n",
    "        if not (sample[1] != sample[1]):\n",
    "            if sample[0] in nc_to_reqID_map.keys()and len(nc_to_reqID_map[sample[0]]) >= 2:\n",
    "                list_of_replacements.append(sample)\n",
    "    list_of_replacements = random.sample(list_of_replacements, 100)\n",
    "    \n",
    "    changed_data_list = data_list.copy()\n",
    "    for r_sample in list_of_replacements:\n",
    "        changed_data_list = replace_phrase_with_abb(nc_to_reqID_map, changed_data_list, r_sample)\n",
    "    \n",
    "    return changed_data_list, list_of_replacements "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
