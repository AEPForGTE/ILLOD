{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "based-challenge",
   "metadata": {},
   "source": [
    "## Script to find an estimate for alpha (abbreviation to term ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "operating-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-nashville",
   "metadata": {},
   "source": [
    "## Load PURE Data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "private-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_data = pd.read_csv('pure_data.CSV', names=[\"dataset\", \"id\", \"req_texts\"], sep='\\t', encoding='utf8')\n",
    "ids = list(pure_data['id'].values)\n",
    "reqs = list(pure_data['req_texts'].values)\n",
    "dataset = list(pure_data['dataset'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-coalition",
   "metadata": {},
   "source": [
    "## Define set of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "decreased-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"the\", \"and\", \"i\", \"for\", \"as\", \"an\", \"a\", \"if\", \"any\", \"all\", \"one\", \"on\", \"new\", \"out\", \"we\", \"to\", \"at\", \"by\", \"from\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-commerce",
   "metadata": {},
   "source": [
    "## Helper functions to extract noun chunks (NCs) and abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "effective-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portion_of_capital_letters(w):\n",
    "    upper_cases = ''.join([c for c in w if c.isupper()])\n",
    "    return len(upper_cases)/len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "tough-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_nc(nc):\n",
    "    doc = nlp(nc)\n",
    "    cleaned_nc = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"DET\":\n",
    "            cleaned_nc = cleaned_nc + \" \" + token.lemma_\n",
    "            cleaned_nc = re.sub(r\"[\\([{})\\]]\", \"\", cleaned_nc)\n",
    "            cleaned_nc = cleaned_nc.strip()\n",
    "    return cleaned_nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-encoding",
   "metadata": {},
   "source": [
    "Extraction of noun chunks according to [2] (Arora, Chetan, et al. \"Automated extraction and clustering of requirements glossary terms.\" IEEE Transactions on Software Engineering 43.10 (2016): 918-945). Some Pos-Tag-Patterns are added to the NC detection to improve recall of spacy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "english-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_detect(req):\n",
    "    noun_chunks_set = set()\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern1 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "    pattern2 = [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    pattern3 = [{'POS': 'NOUN'}, {'POS': 'DET'}, {'POS': 'NOUN'}]\n",
    "    pattern4 = [{'POS': 'NOUN'}]\n",
    "    matcher.add(\"TrigramNCs\", [pattern1, pattern2, pattern3, pattern4])\n",
    "    doc = nlp(req)\n",
    "    matches = matcher(doc)\n",
    "    for nc_ in doc.noun_chunks:\n",
    "        noun_chunks_set.add(nc_.text)\n",
    "    \n",
    "\n",
    "    composed_terms = set()\n",
    "    for nc1 in noun_chunks_set:\n",
    "        for nc2 in noun_chunks_set:\n",
    "            comp_term1 = nc1 + \" of \" + nc2\n",
    "            comp_term2 = nc1 + \" and \" + nc2\n",
    "            if comp_term1 in req:\n",
    "                composed_terms.add(comp_term1)\n",
    "            if comp_term2 in req:\n",
    "                composed_terms.add(comp_term2)\n",
    "    found_terms = noun_chunks_set.union(composed_terms)\n",
    "    \n",
    "    cleaned_terms = []\n",
    "    for t in found_terms:\n",
    "        cleaned_terms.append(normalize_nc(t))\n",
    "    return set(cleaned_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "pleased-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of abbreviations according to the F1-optimized approach\n",
    "def abbv_detect(sent):\n",
    "    abv = set()\n",
    "    for word in sent.split():\n",
    "        if (len(word) <= 13 and portion_of_capital_letters(word) >= 0.29):\n",
    "            if len([c for c in word if c.isupper()]) == 1 and word[0].isupper() and word.lower() in stop_words:\n",
    "                continue\n",
    "            abv.add(word.strip(punctuation))\n",
    "    return abv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-rendering",
   "metadata": {},
   "source": [
    "## The main function: Collect the set of NCs and the set of Abbreviations independantly and compare their length at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "surface-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_detected_ncs = set()\n",
    "for req in reqs:\n",
    "    set_of_detected_ncs = set_of_detected_ncs.union(nc_detect(req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "olympic-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "termination of shunt group call\n",
      "national train control system\n",
      "level of public health\n",
      "time of activity\n",
      "result of self - test\n",
      "upper value of valid temperature range\n",
      "active member of shunt group\n",
      "restart of system\n",
      "Display Name\n",
      "procedure\n",
      "user info\n",
      "av\n",
      "fuel status\n",
      "VSP Code\n",
      "flight and avionic system status\n",
      "datum server\n",
      "provision\n",
      "mission planning monitor\n",
      "salesman and team\n",
      "define limit\n"
     ]
    }
   ],
   "source": [
    "for term in list(set_of_detected_ncs)[:20]:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "needed-berkeley",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3195\n"
     ]
    }
   ],
   "source": [
    "print(len(set_of_detected_ncs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "excess-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_detected_abbreviations = set()\n",
    "for req in reqs:\n",
    "    extracted_abbreviations = abbv_detect(req)\n",
    "    for abbv in extracted_abbreviations:\n",
    "        cleaned_abbv = re.sub(r\"[\\([{})\\]]\", \"\", abbv)\n",
    "        set_of_detected_abbreviations.add(cleaned_abbv.strip(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "affecting-roommate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "print(len(set_of_detected_abbreviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ancient-obligation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.152173913043477\n"
     ]
    }
   ],
   "source": [
    "ratio = len(set_of_detected_ncs)/len(set_of_detected_abbreviations)\n",
    "print(ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
