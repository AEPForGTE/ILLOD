{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annual-huntington",
   "metadata": {},
   "source": [
    "# Evaluation of ILLODs practicability on real sets of requirements (Section 4.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "looking-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-wells",
   "metadata": {},
   "source": [
    "## Reading the content of the PROMISE requirements. 30 abbreviations have been inserted in the texts. We want try to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "rotary-editing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  set_id\n",
      "1    The system shall refresh the display every 60 ...       1\n",
      "2    The application shall match the color of the s...       1\n",
      "3    If projected  the data must be readable.  On a...       1\n",
      "4    The product shall be available during normal b...       1\n",
      "5    If projected  the data must be understandable....       1\n",
      "..                                                 ...     ...\n",
      "621  User access should be limited to the permissio...      15\n",
      "622  The product must comply with the intranet page...      15\n",
      "623   The intranet pages should display appropriate...      15\n",
      "624  The users should be able to easily use the sys...      15\n",
      "625  The product interface should be fast. The resp...      15\n",
      "\n",
      "[625 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "requirements_data = pd.read_csv('promise_constructed.CSV', names=['text', 'set_id'], sep=';', encoding='utf8')\n",
    "print(requirements_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "specific-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the data with the aim of storing it in a dictionary\n",
    "data_dict = {}\n",
    "for id_ in set(requirements_data[\"set_id\"]):\n",
    "    sublist = requirements_data[requirements_data[\"set_id\"] == id_]\n",
    "    data_dict[id_] = [req for req in sublist[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-europe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "organic-victim",
   "metadata": {},
   "source": [
    "## ILLOD with its methods (Section 4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "supposed-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_special_characters(s1, s2):\n",
    "    invalidcharacters = set(string.punctuation)\n",
    "    if any(char in invalidcharacters for char in s1):\n",
    "        s1_ = s1.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        s1_ = s1\n",
    "    if any(char in invalidcharacters for char in s2):\n",
    "        s2_ = s2.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        s2_ = s2\n",
    "    return s1_, s2_\n",
    "\n",
    "def stop_words_handling(term):\n",
    "    splitted_term = term.split()\n",
    "    stop_words = set([\"for\", \"and\", \"of\", \"in\", \"via\", \"be\"])\n",
    "    \n",
    "    if splitted_term[0] in stop_words:\n",
    "        stop_words = stop_words - set([splitted_term[0]])\n",
    "                \n",
    "    for sw in stop_words:\n",
    "        while sw in splitted_term:\n",
    "            splitted_term.remove(sw)\n",
    "    sanitized_term = \" \".join([w for w in splitted_term]) \n",
    "        \n",
    "    return sanitized_term\n",
    "\n",
    "def clean_string_pair_and_reduce_expansion(abb, term):\n",
    "    abb_lower = abb.lower()\n",
    "    term_lower = term.lower()\n",
    "    sanitized_abbv, sanitized_term = clear_special_characters(abb_lower, term_lower) \n",
    "    sanitized_term_without_stopswords = stop_words_handling(sanitized_term)\n",
    "    initial_letters_of_tokens_of_sanitized_term_without_stopswords = ''.join([c[0] for c in sanitized_term_without_stopswords.split()])\n",
    "    return sanitized_abbv, initial_letters_of_tokens_of_sanitized_term_without_stopswords\n",
    "\n",
    "def check_initial_letters(a, t):\n",
    "    initial_letters_of_tokens_of_t = ''.join([c[0] for c in t.split()])\n",
    "    if initial_letters_of_tokens_of_t == a or initial_letters_of_tokens_of_t.upper() == a:\n",
    "        return True\n",
    "    \n",
    "def check_length_consistency(a, t):\n",
    "    length_consistency = False\n",
    "    if len(t.split()) <= len(a):\n",
    "        length_consistency = True\n",
    "    return length_consistency\n",
    "\n",
    "def check_order(a, t):\n",
    "    abbv_reversed = a.lower()[::-1]\n",
    "    term_reversed = t.lower()[::-1]\n",
    "    len_of_term = len(t)\n",
    "    \n",
    "    pos_memory = 0\n",
    "    pos_memory_list = []\n",
    "    order_matching_string_rev = \"\"\n",
    "    \n",
    "    for j, char_from_abbv in enumerate(abbv_reversed):\n",
    "        if j == len(abbv_reversed) - 1 and len(pos_memory_list) > 0 and pos_memory == len(term_reversed):\n",
    "            break\n",
    "        else:\n",
    "            for i, char_from_term in enumerate(term_reversed[pos_memory:]):\n",
    "                if char_from_abbv == char_from_term:\n",
    "                    order_matching_string_rev = order_matching_string_rev + char_from_abbv\n",
    "                    pos_memory = pos_memory + i + 1\n",
    "                    pos_memory_list.append(len_of_term - pos_memory)\n",
    "                    break\n",
    "    if order_matching_string_rev == abbv_reversed:\n",
    "        return True, pos_memory_list[::-1]\n",
    "    else:\n",
    "        return False, []\n",
    "\n",
    "def check_distribution_of_matching_characters(pos_of_chars_list, t):\n",
    "    term_intervals = []\n",
    "    len_of_term = len(t)\n",
    "    i = 0\n",
    "    while i < len_of_term:\n",
    "        sublist = []\n",
    "        j = i\n",
    "        while j < len_of_term and t[j] != \" \":\n",
    "            sublist.append(j)\n",
    "            j = j+ 1\n",
    "        i = j+1\n",
    "        term_intervals.append(sublist)\n",
    "        \n",
    "    splitted_term = t.split()      \n",
    "    \n",
    "    containment_list = []\n",
    "    for i, interval in enumerate(term_intervals):\n",
    "        contanment_sublist = []\n",
    "        for pos in pos_of_chars_list:\n",
    "            if (pos in interval) and (splitted_term[i][0] == t[pos]):\n",
    "                contanment_sublist.append(0)\n",
    "            elif pos in interval:\n",
    "                contanment_sublist.append(interval.index(pos))\n",
    "        if len(contanment_sublist) == 0:\n",
    "            contanment_sublist.append(-1)\n",
    "        containment_list.append(contanment_sublist)\n",
    "    \n",
    "    result_of_distribution_check = False\n",
    "    if len(containment_list) <= 1:\n",
    "        result_of_distribution_check = True\n",
    "    elif len (containment_list) >= 2:\n",
    "        non_zero_count = 0\n",
    "        for sublist in containment_list[1:]:\n",
    "            if len(sublist) == 1 and 0 not in sublist:\n",
    "                non_zero_count += 1\n",
    "        if non_zero_count == 0:\n",
    "            result_of_distribution_check = True\n",
    "    \n",
    "    return result_of_distribution_check\n",
    "\n",
    "\n",
    "def illod(abbv, term, threshold=None):\n",
    "    if (abbv[0].lower() == term[0].lower()):\n",
    "        \n",
    "        \n",
    "        ###################################### Step (a) ##########################################\n",
    "        # check wether initial letters of tokens in t match with the letters in abbreviation\n",
    "        if check_initial_letters(abbv, term):\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################### Step (b) ##########################################\n",
    "        # clean abbreviation and term from special characters and stopwords\n",
    "        a_, t_ = clean_string_pair_and_reduce_expansion(abbv, term)\n",
    "        if a_ == t_:\n",
    "            return True\n",
    "        \n",
    "        sanitized_abbv, sanitized_term = clear_special_characters(abbv, term) \n",
    "        sanitized_term_without_stopswords = stop_words_handling(sanitized_term)\n",
    "        sanitized_term_without_stopswords_splitted  = sanitized_term_without_stopswords.split()\n",
    "        \n",
    "        ###################################### Step (c), (d), (e) #################################\n",
    "        # Sequential call of the methods that check and compare lengths, order and distribution of characters\n",
    "        length_consistency = check_length_consistency(sanitized_abbv, sanitized_term_without_stopswords)\n",
    "        order, pos_of_chars_list = check_order(sanitized_abbv, sanitized_term_without_stopswords)\n",
    "        distribution = check_distribution_of_matching_characters(pos_of_chars_list, sanitized_term_without_stopswords)\n",
    "\n",
    "\n",
    "        if length_consistency and order and distribution:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        ################################## in case first letter differs ###########################\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-arlington",
   "metadata": {},
   "source": [
    "## Helper Functions to extract noun chunks (NCs) and abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "legendary-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"the\", \"and\", \"i\", \"for\", \"as\", \"an\", \"a\", \"if\", \"any\", \"all\", \"one\", \"on\", \"new\", \"out\", \"we\", \"to\", \"at\", \"by\", \"from\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "naval-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_ratio(w):\n",
    "    upper_cases = ''.join([c for c in w if c.isupper()])\n",
    "    return len(upper_cases)/len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "corresponding-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbv_detect(sent):\n",
    "    abv = set()\n",
    "    for word in sent.split():\n",
    "        if (len(word) <= 13 and upper_ratio(word) >= 0.29):\n",
    "            if len([c for c in word if c.isupper()]) == 1 and word[0].isupper() and word.lower() in stop_words:\n",
    "                continue\n",
    "            abv.add(word.strip(punctuation))\n",
    "    return abv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "sharing-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_nc(nc):\n",
    "    doc = nlp(nc)\n",
    "    cleaned_nc = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"DET\":\n",
    "            cleaned_nc = cleaned_nc + \" \" + token.lemma_\n",
    "            cleaned_nc = re.sub(r\"[\\([{})\\]]\", \"\", cleaned_nc)\n",
    "            cleaned_nc = cleaned_nc.strip()\n",
    "    return cleaned_nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-trial",
   "metadata": {},
   "source": [
    "## NC Extraction\n",
    "Extraction of noun chunks according to [2] (Arora, Chetan, et al. \"Automated extraction and clustering of requirements glossary terms.\" IEEE Transactions on Software Engineering 43.10 (2016): 918-945). Some Pos-Tag-Patterns are added to the NC detection to improve recall of spacy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ultimate-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_detect(req):\n",
    "    noun_chunks_set = set()\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern1 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "    pattern2 = [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    pattern3 = [{'POS': 'NOUN'}, {'POS': 'DET'}, {'POS': 'NOUN'}]\n",
    "    pattern4 = [{'POS': 'NOUN'}, {'POS': 'PROPN'}]\n",
    "    pattern5 = [{'POS': 'NOUN'}]\n",
    "    matcher.add(\"TrigramNCs\", [pattern1, pattern2, pattern3, pattern4, pattern5])\n",
    "    doc = nlp(req)\n",
    "    matches = matcher(doc)\n",
    "    for nc_ in doc.noun_chunks:\n",
    "        noun_chunks_set.add(nc_.text)\n",
    "    \n",
    "\n",
    "    composed_terms = set()\n",
    "    for nc1 in noun_chunks_set:\n",
    "        for nc2 in noun_chunks_set:\n",
    "            comp_term1 = nc1 + \" of \" + nc2\n",
    "            comp_term2 = nc1 + \" and \" + nc2\n",
    "            if comp_term1 in req:\n",
    "                composed_terms.add(comp_term1)\n",
    "            if comp_term2 in req:\n",
    "                composed_terms.add(comp_term2)\n",
    "    found_terms = noun_chunks_set.union(composed_terms)\n",
    "    \n",
    "    cleaned_terms = []\n",
    "    for t in found_terms:\n",
    "        cleaned_terms.append(normalize_nc(t))\n",
    "    return set(cleaned_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-cheat",
   "metadata": {},
   "source": [
    "### The following function performs a set transformation. It distributes objects of the sets $A$ and $T$ to the sets $OT$ and $G^{a}$ according to steps (4) and (6) from section (5.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "premier-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sets_for_term_types(set_of_abbreviations, set_of_terms):\n",
    "    \n",
    "    #compliant wit section 5.2: terms_that_contain_abbreviations = T \\ OT\n",
    "    terms_that_contain_abbreviations = set()\n",
    "    \n",
    "    for term in set_of_terms:\n",
    "        for abb in set_of_abbreviations:\n",
    "            if abb in term.split():\n",
    "                terms_that_contain_abbreviations.add(term)\n",
    "    \n",
    "    ordinary_terms = terms - terms_that_contain_abbreviations\n",
    "    \n",
    "    return ordinary_terms, terms_that_contain_abbreviations              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "square-prediction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUPLES FROM ReqSet: 1\n",
      "1) (MDI, modification)\n",
      "2) (MDI, modification of display)\n",
      "3) (cT, current time)\n",
      "4) (cT, cT.)\n",
      "5) (cT, chart)\n",
      "6) (PC, product)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 2\n",
      "7) (CE, client)\n",
      "8) (RT, realtor)\n",
      "9) (SR, search result)\n",
      "10) (SR, seller)\n",
      "11) (CMA, contact information)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 3\n",
      "12) (PoS, portion of system)\n",
      "13) (PoS, Program Administrators)\n",
      "14) (PoS, possibility)\n",
      "15) (Dr, department)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 4\n",
      "16) (NSM, nursing Staff member)\n",
      "17) (Csi, clinical site)\n",
      "18) (E, example)\n",
      "19) (E, exception)\n",
      "20) (ID, industry)\n",
      "21) (In, information)\n",
      "22) (In, illness)\n",
      "23) (In, interface creation)\n",
      "24) (In, industry)\n",
      "25) (In, instructor)\n",
      "26) (TR, transaction and industry)\n",
      "27) (TR, transaction)\n",
      "28) (TR, type of retrieval)\n",
      "29) (TR, type of transaction)\n",
      "30) (RTR, retrieval)\n",
      "31) (DS, dispute system)\n",
      "32) (DS, Disputes System)\n",
      "33) (DS, department / section)\n",
      "34) (DS, dispute)\n",
      "35) (DS, database)\n",
      "36) (DC, dispute case)\n",
      "37) (DC, document)\n",
      "38) (DC, documentation)\n",
      "39) (Lab, labs)\n",
      "40) (Lab, lab)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 5\n",
      "41) (CE, collision estimator)\n",
      "42) (CE, category)\n",
      "43) (CE, collision estimate)\n",
      "44) (CE, claim processing)\n",
      "45) (CE, Collision Estimators)\n",
      "46) (RF, repair facility)\n",
      "47) (AR, adjuster role)\n",
      "48) (AR, adjuster)\n",
      "49) (AR, audit report)\n",
      "50) (AR, appearance)\n",
      "51) (AR, available part)\n",
      "52) (rP, recycled part)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 6\n",
      "53) (HTML, html)\n",
      "54) (MS, meeting schedule)\n",
      "55) (DBMS, database management system)\n",
      "56) (DBMS, dbms)\n",
      "57) (SP, software product)\n",
      "58) (SP, survey respondent)\n",
      "59) (SP, support)\n",
      "60) (SP, search parameter)\n",
      "61) (CR, collaboration)\n",
      "62) (CR, customer)\n",
      "63) (CR, conference room)\n",
      "64) (CD, cr schedule)\n",
      "65) (CD, calendar date)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 7\n",
      "66) (It, inventory)\n",
      "67) (It, independent audit)\n",
      "68) (PF, Product formula)\n",
      "69) (PF, product formula)\n",
      "70) (sI, substitutionary ingredient)\n",
      "71) (IQA, Inventory Quantity Adjustment)\n",
      "72) (PMs, permission)\n",
      "73) (RMS, rms system)\n",
      "74) (POS, part of System)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 8\n",
      "75) (PEAR, prepaid card)\n",
      "76) (PEAR, print paper card)\n",
      "77) (CC, customer of change)\n",
      "78) (CC, compliance)\n",
      "79) (CC, credit card)\n",
      "80) (PIN, payment option)\n",
      "81) (PIN, private information)\n",
      "82) (MSN, movie description)\n",
      "83) (IE, information practice)\n",
      "84) (IE, Internet)\n",
      "85) (IE, IzognMovies)\n",
      "86) (IE, infection)\n",
      "87) (IE, integrity)\n",
      "88) (IE, interface)\n",
      "89) (IE, internet)\n",
      "90) (sMo, select movie)\n",
      "91) (sMo, streaming movie)\n",
      "92) (sMo, symbol)\n",
      "93) (sMo, sale information)\n",
      "94) (Sys, system)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 9\n",
      "95) (LeDA, lead data)\n",
      "96) (LeDA, lead datum)\n",
      "97) (LeSco, lead score)\n",
      "98) (WES, washing process)\n",
      "99) (WES, web service)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 10\n",
      "100) (STAT, status)\n",
      "101) (STAT, start)\n",
      "102) (STAT, start of turn)\n",
      "103) (dG, defensive grid)\n",
      "104) (oP, one player)\n",
      "105) (oP, offensive player)\n",
      "106) (oP, op.)\n",
      "107) (oP, other player)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 11\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 12\n",
      "108) (In, interruption)\n",
      "109) (In, interface)\n",
      "110) (CSR, caller and supervisor)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 13\n",
      "111) (W3, w3)\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 14\n",
      "#####################################################\n",
      "TUPLES FROM ReqSet: 15\n",
      "112) (UAS, User access)\n",
      "113) (UAS, uas system)\n",
      "114) (RFS, rfs system)\n",
      "115) (400MB, 400 mb)\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 1:\n",
      "1) \"MDI\" : ['modification of display', 'MDI form']\n",
      "2) \"cT\" : ['current time', 'cT.', 'chart']\n",
      "3) \"PC\" : ['product']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 2:\n",
      "4) \"RT\" : ['realtor']\n",
      "5) \"SR\" : ['search result', 'seller', 'property SR']\n",
      "6) \"CMA\" : ['contact information', 'CMA report criterion', 'CMA report']\n",
      "7) \"CE\" : ['Windows CE and Palm operating system']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 3:\n",
      "8) \"PoS\" : ['Program Administrators', 'possibility']\n",
      "9) \"Dr\" : ['department', 'Dr Julie Donalek', 'Dr Susan Poslusny and Karen Sysol', 'Dr Susan Poslusny', 'Dr Susan Poslusny and Dr Julie Donalek']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 4:\n",
      "10) \"Csi\" : ['clinical site']\n",
      "11) \"E\" : ['example', 'exception']\n",
      "12) \"ID\" : ['industry', 'student ID student name']\n",
      "13) \"In\" : ['information', 'illness', 'interface creation', 'industry', 'instructor']\n",
      "14) \"TR\" : ['transaction and industry', 'transaction', 'type of retrieval', 'type of transaction', 'part of TR creation process', 'TR request']\n",
      "15) \"RTR\" : ['retrieval']\n",
      "16) \"DS\" : ['dispute system', 'Disputes System', 'department / section', 'dispute', 'database']\n",
      "17) \"DC\" : ['dispute case', 'document', 'documentation']\n",
      "18) \"Lab\" : ['labs', 'lab', 'Clinical Lab Section']\n",
      "19) \"NSM\" : ['Program Administrators and NSM']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 5:\n",
      "20) \"CE\" : ['category', 'collision estimate', 'claim processing', 'Collision Estimators']\n",
      "21) \"RF\" : ['repair facility', 'prefer RF']\n",
      "22) \"AR\" : ['adjuster role', 'adjuster', 'audit report', 'appearance', 'available part']\n",
      "23) \"rP\" : ['recycled part', 'rp and actual use of rP', 'attempt use of rP and actual use', 'attempt use of rP']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 6:\n",
      "24) \"MS\" : ['meeting schedule', 'MS Access']\n",
      "25) \"DBMS\" : ['database management system', 'dbms']\n",
      "26) \"SP\" : ['software product', 'survey respondent', 'support', 'search parameter']\n",
      "27) \"CR\" : ['collaboration', 'customer', 'conference room']\n",
      "28) \"CD\" : ['cr schedule', 'calendar date']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 7:\n",
      "29) \"It\" : ['independent audit']\n",
      "30) \"PF\" : ['Product formula', 'product formula', 'PF substitutionary ingredient']\n",
      "31) \"sI\" : ['substitutionary ingredient']\n",
      "32) \"IQA\" : ['Inventory Quantity Adjustment', 'IQA subsystem', 'IQA document']\n",
      "33) \"PMs\" : ['permission', 'build PMs']\n",
      "34) \"RMS\" : ['rms system']\n",
      "35) \"POS\" : ['part of System', 'POS terminal']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 8:\n",
      "36) \"PEAR\" : ['print paper card', 'PEAR standard']\n",
      "37) \"CC\" : ['customer of change', 'compliance', 'credit card']\n",
      "38) \"PIN\" : ['payment option', 'private information', 'PIN number']\n",
      "39) \"MSN\" : ['movie description', 'MSN TV2']\n",
      "40) \"IE\" : ['information practice', 'Internet', 'IzognMovies', 'infection', 'integrity', 'interface', 'internet']\n",
      "41) \"sMo\" : ['select movie', 'streaming movie', 'symbol', 'sale information']\n",
      "42) \"Sys\" : ['system']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 9:\n",
      "43) \"LeDA\" : ['lead datum', 'LeDA parameter']\n",
      "44) \"LeSco\" : ['lead score']\n",
      "45) \"WES\" : ['washing process', 'web service']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 10:\n",
      "46) \"STAT\" : ['start', 'start of turn', 'STAT of defensive player']\n",
      "47) \"dG\" : ['defensive grid']\n",
      "48) \"oP\" : ['one player', 'offensive player', 'op.', 'other player']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 12:\n",
      "49) \"In\" : ['interface']\n",
      "50) \"CSR\" : ['caller and supervisor', 'CSR and Datastream']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 13:\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 15:\n",
      "51) \"UAS\" : ['uas system']\n",
      "52) \"RFS\" : ['rfs system', 'RFS system']\n",
      "53) \"400MB\" : ['400 mb']\n",
      "#####################################################\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "AEP_candidate_clusters ={}\n",
    "for id_ in data_dict.keys():\n",
    "    \n",
    "    ######### Step(1) + Step(3): Extract set of Abbreviations A and set of terms T############\n",
    "\n",
    "    terms = set()\n",
    "    abbv_set = set()\n",
    "    print(\"TUPLES FROM ReqSet: \" + str(id_))\n",
    "    for req in data_dict[id_]:\n",
    "        terms = terms.union(nc_detect(req))\n",
    "        abbv_set = abbv_set.union(abbv_detect(req))\n",
    "\n",
    "    ############ step(2): Reduce extracted abbreviations set A through cmparision with #######\n",
    "    ############ project resources so that only undefined abbreviations stay in A ############\n",
    "   \n",
    "    \n",
    "\n",
    "    ############################ step(4): determine the sets A, OT and T\\OT ##################\n",
    "    ordinary_terms, terms_that_contain_abbs = determine_sets_for_term_types(abbv_set, terms)\n",
    "    \n",
    "\n",
    "    ###################################### step(5): ##########################################\n",
    "    abbreviations_with_matching_candidates = set()\n",
    "    for abv in abbv_set:\n",
    "        for term in ordinary_terms:\n",
    "            if illod(abv, term):\n",
    "                counter += 1\n",
    "                abbreviations_with_matching_candidates.add(abv)\n",
    "                print(str(counter)+ \") (\" + abv + \", \" + term + \")\")\n",
    "                if id_ in AEP_candidate_clusters.keys():\n",
    "                    if abv in AEP_candidate_clusters[id_]:\n",
    "                        expansion_candidates_list = AEP_candidate_clusters[id_][abv]\n",
    "                        expansion_candidates_list.append(term)\n",
    "                        AEP_candidate_clusters[id_][abv] = expansion_candidates_list\n",
    "                    else:\n",
    "                        AEP_candidate_clusters[id_][abv] = [term]\n",
    "                else:\n",
    "                    AEP_candidate_clusters[id_] = {}\n",
    "    \n",
    "    ######################################### step(6): #######################################\n",
    "    for abv in abbreviations_with_matching_candidates:\n",
    "        for term in terms_that_contain_abbs:\n",
    "            if abv in term.split() and abv != term:\n",
    "                if id_ in AEP_candidate_clusters.keys():\n",
    "                    if abv in AEP_candidate_clusters[id_]:\n",
    "                        expansion_candidates_list = AEP_candidate_clusters[id_][abv]\n",
    "                        expansion_candidates_list.append(term)\n",
    "                        AEP_candidate_clusters[id_][abv] = expansion_candidates_list\n",
    "                    else:\n",
    "                        AEP_candidate_clusters[id_][abv] = [term]\n",
    "                else:\n",
    "                    AEP_candidate_clusters[id_] = {}\n",
    "    print(\"#####################################################\")\n",
    "\n",
    "cluster_counter = 0\n",
    "for id_ in AEP_candidate_clusters.keys():\n",
    "    print(\"CLUSTERS FROM ReqSet: \" + str(id_) + \":\")\n",
    "    for key in AEP_candidate_clusters[id_]:\n",
    "        cluster_counter += 1\n",
    "        print(str(cluster_counter) + \") \" + \"\\\"\" + str(key)+ \"\\\"\" + \" : \" + str(AEP_candidate_clusters[id_][key]))\n",
    "    print(\"#####################################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
