{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annual-huntington",
   "metadata": {},
   "source": [
    "# Evaluation of Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "looking-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rotary-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_data = pd.read_csv('promise_constructed.CSV', names=['text', 'set_id'], sep=';', encoding='utf8')\n",
    "reqs = list(requirements_data['text'].values)\n",
    "list_of_id = list(requirements_data['set_id'].values)\n",
    "data_dict = {}\n",
    "for j, id_ in enumerate(list_of_id):\n",
    "    if id_ not in data_dict.keys(): \n",
    "        data_dict[id_] = [reqs[j]]\n",
    "    else:\n",
    "        tmp_list = data_dict[id_]\n",
    "        tmp_list.append(reqs[j])\n",
    "        data_dict[id_] = tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "constitutional-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"the\", \"and\", \"i\", \"for\", \"as\", \"an\", \"a\", \"if\", \"any\", \"all\", \"one\", \"on\", \"new\", \"out\", \"we\", \"to\", \"at\", \"by\", \"from\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monthly-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_ratio(w):\n",
    "    upper_cases = ''.join([c for c in w if c.isupper()])\n",
    "    return len(upper_cases)/len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "supposed-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_special_characters(s1, s2):\n",
    "    invalidcharacters = set(string.punctuation)\n",
    "    if any(char in invalidcharacters for char in s1):\n",
    "        s1_ = s1.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        s1_ = s1\n",
    "    if any(char in invalidcharacters for char in s2):\n",
    "        s2_ = s2.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        s2_ = s2\n",
    "    return s1_, s2_\n",
    "\n",
    "def stop_words_handling(term):\n",
    "    splitted_term = term.split()\n",
    "    stop_words = set([\"for\", \"and\", \"of\", \"in\", \"via\", \"be\"])\n",
    "    \n",
    "    if splitted_term[0] in stop_words:\n",
    "        stop_words = stop_words - set([splitted_term[0]])\n",
    "                \n",
    "    for sw in stop_words:\n",
    "        while sw in splitted_term:\n",
    "            splitted_term.remove(sw)\n",
    "    sanitized_term = \" \".join([w for w in splitted_term]) \n",
    "        \n",
    "    return sanitized_term\n",
    "\n",
    "def clean_string_pair_and_reduce_expansion(abb, term):\n",
    "    abb_lower = abb.lower()\n",
    "    term_lower = term.lower()\n",
    "    sanitized_abbv, sanitized_term = clear_special_characters(abb_lower, term_lower) \n",
    "    sanitized_term_without_stopswords = stop_words_handling(sanitized_term)\n",
    "    initial_letters_of_tokens_of_sanitized_term_without_stopswords = ''.join([c[0] for c in sanitized_term_without_stopswords.split()])\n",
    "    return sanitized_abbv, initial_letters_of_tokens_of_sanitized_term_without_stopswords\n",
    "\n",
    "def check_initial_letters(a, t):\n",
    "    initial_letters_of_tokens_of_t = ''.join([c[0] for c in t.split()])\n",
    "    if initial_letters_of_tokens_of_t == a or initial_letters_of_tokens_of_t.upper() == a:\n",
    "        return True\n",
    "    \n",
    "def check_length_consistency(a, t):\n",
    "    length_consistency = False\n",
    "    if len(t.split()) <= len(a):\n",
    "        length_consistency = True\n",
    "    return length_consistency\n",
    "\n",
    "def check_order(a, t):\n",
    "    abbv_reversed = a.lower()[::-1]\n",
    "    term_reversed = t.lower()[::-1]\n",
    "    len_of_term = len(t)\n",
    "    \n",
    "    pos_memory = 0\n",
    "    pos_memory_list = []\n",
    "    order_matching_string_rev = \"\"\n",
    "    \n",
    "    for j, char_from_abbv in enumerate(abbv_reversed):\n",
    "        if j == len(abbv_reversed) - 1 and len(pos_memory_list) > 0 and pos_memory == len(term_reversed):\n",
    "            break\n",
    "        else:\n",
    "            for i, char_from_term in enumerate(term_reversed[pos_memory:]):\n",
    "                if char_from_abbv == char_from_term:\n",
    "                    order_matching_string_rev = order_matching_string_rev + char_from_abbv\n",
    "                    pos_memory = pos_memory + i + 1\n",
    "                    pos_memory_list.append(len_of_term - pos_memory)\n",
    "                    break\n",
    "    if order_matching_string_rev == abbv_reversed:\n",
    "        return True, pos_memory_list[::-1]\n",
    "    else:\n",
    "        return False, []\n",
    "\n",
    "def check_distribution_of_matching_characters(pos_of_chars_list, t):\n",
    "    term_intervals = []\n",
    "    len_of_term = len(t)\n",
    "    i = 0\n",
    "    while i < len_of_term:\n",
    "        sublist = []\n",
    "        j = i\n",
    "        while j < len_of_term and t[j] != \" \":\n",
    "            sublist.append(j)\n",
    "            j = j+ 1\n",
    "        i = j+1\n",
    "        term_intervals.append(sublist)\n",
    "        \n",
    "    splitted_term = t.split()      \n",
    "    \n",
    "    containment_list = []\n",
    "    for i, interval in enumerate(term_intervals):\n",
    "        contanment_sublist = []\n",
    "        for pos in pos_of_chars_list:\n",
    "            if (pos in interval) and (splitted_term[i][0] == t[pos]):\n",
    "                contanment_sublist.append(0)\n",
    "            elif pos in interval:\n",
    "                contanment_sublist.append(interval.index(pos))\n",
    "        if len(contanment_sublist) == 0:\n",
    "            contanment_sublist.append(-1)\n",
    "        containment_list.append(contanment_sublist)\n",
    "    \n",
    "    result_of_distribution_check = False\n",
    "    if len(containment_list) <= 1:\n",
    "        result_of_distribution_check = True\n",
    "    elif len (containment_list) >= 2:\n",
    "        non_zero_count = 0\n",
    "        for sublist in containment_list[1:]:\n",
    "            if len(sublist) == 1 and 0 not in sublist:\n",
    "                non_zero_count += 1\n",
    "        if non_zero_count == 0:\n",
    "            result_of_distribution_check = True\n",
    "    \n",
    "    return result_of_distribution_check\n",
    "\n",
    "\n",
    "def illod(abbv, term, threshold=None):\n",
    "    if (abbv[0].lower() == term[0].lower()):\n",
    "        \n",
    "        \n",
    "        ###################################### Step (a) ##########################################\n",
    "        # check wether initial letters of tokens in t match with the letters in abbreviation\n",
    "        if check_initial_letters(abbv, term):\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################### Step (b) ########################################\n",
    "        # clean abbreviation and term from special characters and stopwords\n",
    "        a_, t_ = clean_string_pair_and_reduce_expansion(abbv, term)\n",
    "        if a_ == t_:\n",
    "            return True\n",
    "        \n",
    "        sanitized_abbv, sanitized_term = clear_special_characters(abbv, term) \n",
    "        sanitized_term_without_stopswords = stop_words_handling(sanitized_term)\n",
    "        sanitized_term_without_stopswords_splitted  = sanitized_term_without_stopswords.split()\n",
    "        \n",
    "        ###################################### Step (c) ##########################################\n",
    "        # Sequential call of the methods that check and compare lengths, order and distribution of characters\n",
    "        length_consistency = check_length_consistency(sanitized_abbv, sanitized_term_without_stopswords)\n",
    "        order, pos_of_chars_list = check_order(sanitized_abbv, sanitized_term_without_stopswords)\n",
    "        distribution = check_distribution_of_matching_characters(pos_of_chars_list, sanitized_term_without_stopswords)\n",
    "\n",
    "\n",
    "        if length_consistency and order and distribution:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        ################################## in case nothing matches #################################\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "naval-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_nc(nc):\n",
    "    doc = nlp(nc)\n",
    "    cleaned_nc = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"DET\":\n",
    "            cleaned_nc = cleaned_nc + \" \" + token.lemma_\n",
    "            cleaned_nc = cleaned_nc.strip()\n",
    "    return cleaned_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corresponding-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbv_detect(sent):\n",
    "    abv = set()\n",
    "    for word in sent.split():\n",
    "        if (len(word) <= 13 and upper_ratio(word) >= 0.29):\n",
    "            if len([c for c in word if c.isupper()]) == 1 and word[0].isupper() and word.lower() in stop_words:\n",
    "                continue\n",
    "            abv.add(word)\n",
    "    return abv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ultimate-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_detect(req):\n",
    "    noun_chunks_set = set()\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern1 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "    pattern2 = [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    pattern3 = [{'POS': 'NOUN'}, {'POS': 'DET'}, {'POS': 'NOUN'}]\n",
    "    pattern4 = [{'POS': 'NOUN'}]\n",
    "    matcher.add(\"TrigramNCs\", [pattern1, pattern2, pattern3, pattern4])\n",
    "    doc = nlp(req)\n",
    "    matches = matcher(doc)\n",
    "    for nc_ in doc.noun_chunks:\n",
    "        noun_chunks_set.add(nc_.text)\n",
    "    \n",
    "\n",
    "    composed_terms = set()\n",
    "    for nc1 in noun_chunks_set:\n",
    "        for nc2 in noun_chunks_set:\n",
    "            comp_term1 = nc1 + \" of \" + nc2\n",
    "            comp_term2 = nc1 + \" and \" + nc2\n",
    "            if comp_term1 in req:\n",
    "                composed_terms.add(comp_term1)\n",
    "            if comp_term2 in req:\n",
    "                composed_terms.add(comp_term2)\n",
    "    found_terms = noun_chunks_set.union(composed_terms)\n",
    "    \n",
    "    cleaned_terms = []\n",
    "    for t in found_terms:\n",
    "        cleaned_terms.append(normalize_nc(t))\n",
    "    return set(cleaned_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "premier-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sets_for_term_types(abbv_set, ordinary_terms):\n",
    "    ordinary_terms_without_abbs = ordinary_terms\n",
    "    terms_that_contain_abbs = {}\n",
    "    for abb in abbv_set:\n",
    "        set_of_terms_that_contains_given_abb = set()\n",
    "        for t in ordinary_terms:\n",
    "            if abb in t.split():\n",
    "                set_of_terms_that_contains_given_abb.add(t)\n",
    "        ordinary_terms_without_abbs = ordinary_terms_without_abbs - set_of_terms_that_contains_given_abb\n",
    "        terms_that_contain_abbs[abb] = set_of_terms_that_contains_given_abb\n",
    "    return abbv_set, ordinary_terms_without_abbs, terms_that_contain_abbs              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "square-prediction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUPLES FROM ReqSet: 1\n",
      "1) (cT., chart)\n",
      "2) (cT., current time)\n",
      "3) (MDI, modification of display)\n",
      "4) (MDI, modification)\n",
      "5) (PC, product)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 2\n",
      "6) (RT, realtor)\n",
      "7) (CMA, contact information)\n",
      "8) (CE, client)\n",
      "9) (SR., search result)\n",
      "10) (SR., seller)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 3\n",
      "11) (Dr, department)\n",
      "12) (PoS., portion of system)\n",
      "13) (PoS., possibility)\n",
      "14) (PoS., Program Administrators)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 4\n",
      "15) (DC., documentation)\n",
      "16) (DC., dispute case)\n",
      "17) (DC., document)\n",
      "18) (DS, dispute)\n",
      "19) (DS, dispute system)\n",
      "20) (DS, database)\n",
      "21) (DS, Disputes System)\n",
      "22) (DS, department / section)\n",
      "23) (E, exception)\n",
      "24) (E, example)\n",
      "25) (TR, transaction)\n",
      "26) (TR, transaction and industry)\n",
      "27) (TR, type of retrieval)\n",
      "28) (TR, type of transaction)\n",
      "29) (In, instructor)\n",
      "30) (In, industry)\n",
      "31) (In, illness)\n",
      "32) (In, information)\n",
      "33) (In, interface creation)\n",
      "34) (NSM, nursing Staff member)\n",
      "35) (Lab, labs)\n",
      "36) (Lab, lab)\n",
      "37) (RTR, retrieval)\n",
      "38) (DC, documentation)\n",
      "39) (DC, dispute case)\n",
      "40) (DC, document)\n",
      "41) (Csi, clinical site)\n",
      "42) (ID, industry)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 5\n",
      "43) (RF, repair facility)\n",
      "44) (AR, audit report)\n",
      "45) (AR, adjuster role)\n",
      "46) (AR, appearance)\n",
      "47) (AR, adjuster)\n",
      "48) (AR, available part)\n",
      "49) (rP, recycled part)\n",
      "50) (CE., collision estimate)\n",
      "51) (CE., Collision Estimators)\n",
      "52) (CE., collision estimator)\n",
      "53) (CE., claim processing)\n",
      "54) (CE., category)\n",
      "55) (CE., CE)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 6\n",
      "56) (CR, customer)\n",
      "57) (CR, conference room)\n",
      "58) (CR, collaboration)\n",
      "59) (CD., calendar date)\n",
      "60) (CD., cr schedule)\n",
      "61) (HTML, html)\n",
      "62) (MS, meeting schedule)\n",
      "63) (SP., SP)\n",
      "64) (SP., software product)\n",
      "65) (SP., support)\n",
      "66) (SP., search parameter)\n",
      "67) (SP., survey respondent)\n",
      "68) (DBMS, database management system)\n",
      "69) (DBMS, dbms)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 7\n",
      "70) (PMs, permission)\n",
      "71) (POS, part of System)\n",
      "72) (PF, product formula)\n",
      "73) (PF, Product formula)\n",
      "74) (sI., substitutionary ingredient)\n",
      "75) (RMS, rms system)\n",
      "76) (IQA, Inventory Quantity Adjustment)\n",
      "77) (It, inventory)\n",
      "78) (It, independent audit)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 8\n",
      "79) (sMo, sale information)\n",
      "80) (sMo, streaming movie)\n",
      "81) (sMo, select movie)\n",
      "82) (sMo, symbol)\n",
      "83) (PIN, private information)\n",
      "84) (PIN, payment option)\n",
      "85) (IE, internet)\n",
      "86) (IE, information practice)\n",
      "87) (IE, Internet)\n",
      "88) (IE, integrity)\n",
      "89) (IE, infection)\n",
      "90) (IE, interface)\n",
      "91) (IE, IzognMovies)\n",
      "92) (CC, credit card)\n",
      "93) (CC, customer of change)\n",
      "94) (CC, compliance)\n",
      "95) (PEAR, prepaid card)\n",
      "96) (PEAR, print paper card)\n",
      "97) (Sys, system)\n",
      "98) (MSN, movie description)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 9\n",
      "99) (LeDA, lead data)\n",
      "100) (LeDA, lead datum)\n",
      "101) (WES., WES)\n",
      "102) (WES., web service)\n",
      "103) (WES., washing process)\n",
      "104) (LeSco, lead score)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 10\n",
      "105) (STAT, status)\n",
      "106) (STAT, start)\n",
      "107) (STAT, start of turn)\n",
      "108) (dG, defensive grid)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 11\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 12\n",
      "109) (In, interface)\n",
      "110) (In, interruption)\n",
      "111) (CSR, caller and supervisor)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 13\n",
      "112) (W3, w3)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 14\n",
      "113) (New, network)\n",
      "###################################################################################\n",
      "TUPLES FROM ReqSet: 15\n",
      "114) (RFS, rfs system)\n",
      "115) (UAS, User access)\n",
      "116) (UAS, uas system)\n",
      "117) (400MB., 400 mb)\n",
      "###################################################################################\n",
      "CLUSTERS FROM ReqSet: 1:\n",
      "1) \"cT.\" : ['current time']\n",
      "2) \"MDI\" : ['modification of display', 'modification']\n",
      "3) \"PC\" : ['product']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 2:\n",
      "4) \"CMA\" : ['contact information']\n",
      "5) \"CE\" : ['client']\n",
      "6) \"SR.\" : ['search result', 'seller']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 3:\n",
      "7) \"PoS.\" : ['portion of system', 'possibility', 'Program Administrators']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 4:\n",
      "8) \"DC.\" : ['dispute case', 'document']\n",
      "9) \"DS\" : ['dispute', 'dispute system', 'database', 'Disputes System', 'department / section']\n",
      "10) \"E\" : ['exception', 'example']\n",
      "11) \"TR\" : ['transaction', 'transaction and industry', 'type of retrieval', 'type of transaction']\n",
      "12) \"In\" : ['instructor', 'industry', 'illness', 'information', 'interface creation']\n",
      "13) \"NSM\" : ['nursing Staff member']\n",
      "14) \"Lab\" : ['labs', 'lab']\n",
      "15) \"RTR\" : ['retrieval']\n",
      "16) \"DC\" : ['documentation', 'dispute case', 'document']\n",
      "17) \"Csi\" : ['clinical site']\n",
      "18) \"ID\" : ['industry']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 5:\n",
      "19) \"AR\" : ['audit report', 'adjuster role', 'appearance', 'adjuster', 'available part']\n",
      "20) \"rP\" : ['recycled part']\n",
      "21) \"CE.\" : ['collision estimate', 'Collision Estimators', 'collision estimator', 'claim processing', 'category', 'CE']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 6:\n",
      "22) \"CR\" : ['conference room', 'collaboration']\n",
      "23) \"CD.\" : ['calendar date', 'cr schedule']\n",
      "24) \"HTML\" : ['html']\n",
      "25) \"MS\" : ['meeting schedule']\n",
      "26) \"SP.\" : ['SP', 'software product', 'support', 'search parameter', 'survey respondent']\n",
      "27) \"DBMS\" : ['database management system', 'dbms']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 7:\n",
      "28) \"POS\" : ['part of System']\n",
      "29) \"PF\" : ['product formula', 'Product formula']\n",
      "30) \"sI.\" : ['substitutionary ingredient']\n",
      "31) \"RMS\" : ['rms system']\n",
      "32) \"IQA\" : ['Inventory Quantity Adjustment']\n",
      "33) \"It\" : ['inventory', 'independent audit']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 8:\n",
      "34) \"sMo\" : ['streaming movie', 'select movie', 'symbol']\n",
      "35) \"PIN\" : ['private information', 'payment option']\n",
      "36) \"IE\" : ['internet', 'information practice', 'Internet', 'integrity', 'infection', 'interface', 'IzognMovies']\n",
      "37) \"CC\" : ['credit card', 'customer of change', 'compliance']\n",
      "38) \"PEAR\" : ['prepaid card', 'print paper card']\n",
      "39) \"Sys\" : ['system']\n",
      "40) \"MSN\" : ['movie description']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 9:\n",
      "41) \"LeDA\" : ['lead datum']\n",
      "42) \"WES.\" : ['WES', 'web service', 'washing process']\n",
      "43) \"LeSco\" : ['lead score']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 10:\n",
      "44) \"STAT\" : ['start', 'start of turn']\n",
      "45) \"dG\" : ['defensive grid']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 12:\n",
      "46) \"In\" : ['interruption']\n",
      "47) \"CSR\" : ['caller and supervisor']\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 13:\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 14:\n",
      "#####################################################\n",
      "CLUSTERS FROM ReqSet: 15:\n",
      "48) \"UAS\" : ['User access', 'uas system']\n",
      "49) \"400MB.\" : ['400 mb']\n",
      "#####################################################\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "AEP_candidate_clusters ={}\n",
    "for id_ in data_dict.keys():\n",
    "    ordinary_terms = set()\n",
    "    abbv_set = set()\n",
    "    print(\"TUPLES FROM ReqSet: \" + str(id_))\n",
    "    for req in data_dict[id_]:\n",
    "        ordinary_terms = ordinary_terms.union(nc_detect(req))\n",
    "        abbv_set = abbv_set.union(abbv_detect(req))\n",
    "    \n",
    "    abbv_set, ordinary_terms, terms_that_contain_abbs = determine_sets_for_term_types(abbv_set, ordinary_terms)\n",
    "    \n",
    "    for abv in abbv_set:\n",
    "        for term in ordinary_terms:\n",
    "            if illod(abv, term):\n",
    "                counter += 1\n",
    "                print(str(counter)+ \") (\" + abv + \", \" + term + \")\")\n",
    "                if id_ in AEP_candidate_clusters.keys():\n",
    "                    if abv in AEP_candidate_clusters[id_]:\n",
    "                        expansion_candidates_list = AEP_candidate_clusters[id_][abv]\n",
    "                        expansion_candidates_list.append(term)\n",
    "                        AEP_candidate_clusters[id_][abv] = expansion_candidates_list\n",
    "                    else:\n",
    "                        AEP_candidate_clusters[id_][abv] = [term]\n",
    "                else:\n",
    "                    AEP_candidate_clusters[id_] = {}\n",
    "                \n",
    "                \n",
    "    print(\"###################################################################################\")\n",
    "\n",
    "cluster_counter = 0\n",
    "for id_ in AEP_candidate_clusters.keys():\n",
    "    print(\"CLUSTERS FROM ReqSet: \" + str(id_) + \":\")\n",
    "    for key in AEP_candidate_clusters[id_]:\n",
    "        cluster_counter += 1\n",
    "        print(str(cluster_counter) + \") \" + \"\\\"\" + str(key)+ \"\\\"\" + \" : \" + str(AEP_candidate_clusters[id_][key]))\n",
    "    print(\"#####################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Terms that contain undefined Abbreviations? ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "freelance-rochester",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 and 30 mile\n",
      "1 month\n",
      "10 000 concurrent user\n",
      "10 second\n",
      "15 second\n",
      "1500 user\n",
      "2 year\n",
      "2 year of initial launch\n",
      "30 mile\n",
      "5 second\n",
      "8 second\n",
      "80 %\n",
      "80 % of Collision Estimators\n",
      "85 %\n",
      "85 % of user\n",
      "90 %\n",
      "90 % of system\n",
      "95 %\n",
      "95 % of adjuster\n",
      "98 %\n",
      "98 % of schedule outage\n",
      "98 % uptime\n",
      "AR\n",
      "CE\n",
      "Choice part System\n",
      "ChoiceParts system\n",
      "Collision Estimators\n",
      "Mozilla Firefox\n",
      "Sarbanes - Oxley\n",
      "User help\n",
      "access\n",
      "adjuster\n",
      "adjuster and Collision Estimators\n",
      "adjuster role\n",
      "appearance\n",
      "appearance of product\n",
      "application\n",
      "approximately 1:00 am\n",
      "attempt use\n",
      "attempt use of rP\n",
      "attempt use of rP and actual use\n",
      "audit\n",
      "audit report\n",
      "availability schedule\n",
      "available online time\n",
      "available part\n",
      "available recycled part\n",
      "available recycled part and collision estimate\n",
      "available recycled part and supplier\n",
      "available recycled part information\n",
      "available recycled part information and supplier\n",
      "average number\n",
      "average number of recycled part record\n",
      "blank set\n",
      "blank set of rating\n",
      "category\n",
      "city\n",
      "claim processing\n",
      "collision estimate\n",
      "collision estimator\n",
      "collision estimator role\n",
      "computer virus\n",
      "corporate Architecture guideline\n",
      "corporate User Interface Guidelines\n",
      "corporate color scheme\n",
      "corporate online availability schedule\n",
      "corporate support center\n",
      "current repair facility rating\n",
      "damaged vehicle part information\n",
      "day\n",
      "denial\n",
      "denial of service\n",
      "dirty and noisy condition\n",
      "environment\n",
      "establish corporate maintenance window\n",
      "establish launch time frame\n",
      "estimate\n",
      "estimate assignment\n",
      "estimatic law\n",
      "estimator\n",
      "exist hardware\n",
      "feed\n",
      "feed of recycled part datum\n",
      "high rating\n",
      "indivual line item\n",
      "initial launch\n",
      "input criterion\n",
      "instruction\n",
      "insurance companys claim datum\n",
      "insurance regulation\n",
      "internet explorer\n",
      "invalid datum\n",
      "later point\n",
      "list\n",
      "list of preferred part supplier\n",
      "list of repair facility\n",
      "maintenance\n",
      "maintenance of product\n",
      "malicious attack\n",
      "middleware technology team\n",
      "miles\n",
      "more than 2 %\n",
      "more than 2 % of available online time\n",
      "new rating\n",
      "no long than 15 second\n",
      "number\n",
      "number of available recycled part\n",
      "one insurance company\n",
      "only adjuster\n",
      "only collision estimator\n",
      "only valid datum\n",
      "original search result\n",
      "other adjuster\n",
      "part\n",
      "percentage\n",
      "percentage of available recycled part\n",
      "prefer RF\n",
      "preferred part supplier\n",
      "preferred repair facility\n",
      "preferred repair facility rating\n",
      "product\n",
      "product installation\n",
      "product installation and upgrade\n",
      "productivity\n",
      "productivity of Collision Estimators\n",
      "rP\n",
      "radius\n",
      "radius of 30 mile\n",
      "rating\n",
      "recycled part\n",
      "recycled part audit\n",
      "recycled part audit of collision estimate\n",
      "recycled part audit report\n",
      "recycled part audits\n",
      "recycled part datum\n",
      "recycled part record\n",
      "recycled part search result\n",
      "repair facility\n",
      "repair facility rating\n",
      "rp and actual use\n",
      "rp and actual use of rP\n",
      "save\n",
      "scale\n",
      "schedule outage\n",
      "search\n",
      "search radius\n",
      "search result\n",
      "select recycled part\n",
      "service\n",
      "state\n",
      "street address\n",
      "supervisor role\n",
      "supplied vehicle part\n",
      "supplied vehicle part and supplier\n",
      "supplier\n",
      "system\n",
      "time\n",
      "total number\n",
      "total number of recycled part\n",
      "total score\n",
      "total score of audit\n",
      "training\n",
      "two day\n",
      "up to 1500 simultaneous user\n",
      "upgrade\n",
      "user\n",
      "vehicle datum\n",
      "vehicle location\n",
      "vehicle vehicle location\n",
      "vehicle year\n",
      "zipcode\n"
     ]
    }
   ],
   "source": [
    "ordinary_terms = set()\n",
    "abbv_set = set()\n",
    "for req in data_dict[5]:\n",
    "        ordinary_terms = ordinary_terms.union(nc_detect(req))\n",
    "        abbv_set = abbv_set.union(abbv_detect(req))\n",
    "list_ = sorted(list(ordinary_terms))\n",
    "for term in list_:\n",
    "    print(term)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
