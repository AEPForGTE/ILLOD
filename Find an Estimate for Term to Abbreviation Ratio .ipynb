{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "based-challenge",
   "metadata": {},
   "source": [
    "## Script to find an estimate for alpha (abbreviation to term ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "operating-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-nashville",
   "metadata": {},
   "source": [
    "## Load PURE Data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "private-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_data = pd.read_csv('pure_data.CSV', names=[\"dataset\", \"id\", \"req_texts\"], sep='\\t', encoding='utf8')\n",
    "ids = list(pure_data['id'].values)\n",
    "reqs = list(pure_data['req_texts'].values)\n",
    "dataset = list(pure_data['dataset'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-coalition",
   "metadata": {},
   "source": [
    "## Define set of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "decreased-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"the\", \"and\", \"i\", \"for\", \"as\", \"an\", \"a\", \"if\", \"any\", \"all\", \"one\", \"on\", \"new\", \"out\", \"we\", \"to\", \"at\", \"by\", \"from\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-commerce",
   "metadata": {},
   "source": [
    "## Helper functions to extract noun chunks (NCs) and abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "effective-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_ratio(w):\n",
    "    upper_cases = ''.join([c for c in w if c.isupper()])\n",
    "    return len(upper_cases)/len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "tough-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_nc(nc):\n",
    "    doc = nlp(nc)\n",
    "    cleaned_nc = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"DET\":\n",
    "            cleaned_nc = cleaned_nc + \" \" + token.lemma_\n",
    "            cleaned_nc = re.sub(r\"[\\([{})\\]]\", \"\", cleaned_nc)\n",
    "            cleaned_nc = cleaned_nc.strip()\n",
    "    return cleaned_nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-encoding",
   "metadata": {},
   "source": [
    "Extraction of noun chunks according to [2] (Arora, Chetan, et al. \"Automated extraction and clustering of requirements glossary terms.\" IEEE Transactions on Software Engineering 43.10 (2016): 918-945). Some Pos-Tag-Patterns are added to the NC detection to improve recall of spacy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "english-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_detect(req):\n",
    "    noun_chunks_set = set()\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern1 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "    pattern2 = [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    pattern3 = [{'POS': 'NOUN'}, {'POS': 'DET'}, {'POS': 'NOUN'}]\n",
    "    pattern4 = [{'POS': 'NOUN'}]\n",
    "    matcher.add(\"TrigramNCs\", [pattern1, pattern2, pattern3, pattern4])\n",
    "    doc = nlp(req)\n",
    "    matches = matcher(doc)\n",
    "    for nc_ in doc.noun_chunks:\n",
    "        noun_chunks_set.add(nc_.text)\n",
    "    \n",
    "\n",
    "    composed_terms = set()\n",
    "    for nc1 in noun_chunks_set:\n",
    "        for nc2 in noun_chunks_set:\n",
    "            comp_term1 = nc1 + \" of \" + nc2\n",
    "            comp_term2 = nc1 + \" and \" + nc2\n",
    "            if comp_term1 in req:\n",
    "                composed_terms.add(comp_term1)\n",
    "            if comp_term2 in req:\n",
    "                composed_terms.add(comp_term2)\n",
    "    found_terms = noun_chunks_set.union(composed_terms)\n",
    "    \n",
    "    cleaned_terms = []\n",
    "    for t in found_terms:\n",
    "        cleaned_terms.append(normalize_nc(t))\n",
    "    return set(cleaned_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "pleased-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of abbreviations according to the F1-optimized approach\n",
    "def abbv_detect(sent):\n",
    "    abv = set()\n",
    "    for word in sent.split():\n",
    "        if (len(word) <= 13 and upper_ratio(word) >= 0.29):\n",
    "            if len([c for c in word if c.isupper()]) == 1 and word[0].isupper() and word.lower() in stop_words:\n",
    "                continue\n",
    "            abv.add(word.strip(punctuation))\n",
    "    return abv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-rendering",
   "metadata": {},
   "source": [
    "## The main function: Collect the set of NCs and the set of Abbreviations independantly and compare their length at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "surface-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_detected_ncs = set()\n",
    "for req in reqs:\n",
    "    set_of_detected_ncs = set_of_detected_ncs.union(nc_detect(req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "olympic-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of file\n",
      "programmable parameter\n",
      "way\n",
      "visible indication\n",
      "special MMI action\n",
      "primary CDN\n",
      "link assurance tone\n",
      "default\n",
      "reversing function\n",
      "intend recipient\n",
      "food\n",
      "c4i system\n",
      "alphanumeric description of identity\n",
      "estimate of error\n",
      "vcd transportation system\n",
      "exposure level\n",
      "number of different source\n",
      "access restriction\n",
      "permission\n",
      "provisioning and reservation\n"
     ]
    }
   ],
   "source": [
    "for term in list(set_of_detected_ncs)[:20]:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "needed-berkeley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3195\n"
     ]
    }
   ],
   "source": [
    "print(len(set_of_detected_ncs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "excess-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_detected_abbreviations = set()\n",
    "for req in reqs:\n",
    "    extracted_abbreviations = abbv_detect(req)\n",
    "    for abbv in extracted_abbreviations:\n",
    "        cleaned_abbv = re.sub(r\"[\\([{})\\]]\", \"\", abbv)\n",
    "        set_of_detected_abbreviations.add(cleaned_abbv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "empirical-quality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTH-Gold\n",
      "IFF\n",
      "CD-ROM\n",
      "IR\n",
      "CPU\n",
      "DMI\n",
      "LOINC\n",
      "non-EIRENE\n",
      "SHOULD/MUST\n",
      "Of\n",
      "VGCS\n",
      "LOS\n",
      "VHF\n",
      "ADT\n",
      "Epi\n",
      "CDNs\n",
      "DFAD\n",
      "HWCIs\n",
      "PAs\n",
      "AVs\n"
     ]
    }
   ],
   "source": [
    "for abbv in list(set_of_detected_abbreviations)[:20]:\n",
    "    print(abbv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "affecting-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "print(len(set_of_detected_abbreviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ancient-obligation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.152173913043477\n"
     ]
    }
   ],
   "source": [
    "ratio = (len(set_of_detected_ncs) - len(set_of_detected_abbreviations))/len(set_of_detected_abbreviations)\n",
    "print(ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
